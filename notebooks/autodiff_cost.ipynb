{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of the cost of calculation of the autodiff calibration\n",
    "After the first release of Tunax, we faced a problem about the cost of calculation of the calibration which was way too heavy. The cost of calculation of an experience of calibration comes from the computation of the gradient of the cost function with the autodifferentiation of JAX. Indeed, a part of this cost function is the whole forward model itself. The model itself, taking as a function, is composed of a large amount of operations taking in account all the temporal steps of integration. That's why the gradient is too expensive to compute : the memory of all the operations that it does is too large. In this notebook we will study with a simplified model how to use the JAX autodiff to compute the gradient of a cost function with a reasonable cost of calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure, Axes\n",
    "from typing import List, Tuple, TypeAlias\n",
    "subplot_1D_type: TypeAlias = Tuple[Figure, List[Axes]]\n",
    "subplot_2D_type: TypeAlias = Tuple[Figure, List[List[Axes]]]\n",
    "latex_installed = shutil.which(\"latex\") is not None\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': latex_installed,\n",
    "    'figure.figsize': (8, 5),\n",
    "    'axes.titlesize': 18,\n",
    "    'figure.titlesize': 18,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'lines.linewidth': 2,\n",
    "    'lines.markersize': 6\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "from typing import Tuple\n",
    "from jax import lax, jit, random\n",
    "from time import time as tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on JAX\n",
    "##### `scan` vs. `fori_loop`\n",
    "`scan` est le plus deep\n",
    "\n",
    "If the trip count is static (meaning known at tracing time, perhaps because lower and upper are Python integer literals) then the fori_loop is implemented in terms of scan() and reverse-mode autodiff is supported; otherwise, a while_loop is used and reverse-mode autodiff is not supported. See those functions’ docstrings for more information.\n",
    "\n",
    "Si les itérations sont indépendantes, fori_loop est plus efficace. Pour des dépendances complexes, scan est préférable mais plus gourmand.\n",
    "\n",
    "En fait scan permet la sortie des états intermédiaires, donc utile pour une intégration temporelle par exemple alors que fori_loop c'est plus pour une boucle itérative simple (et elle y est plus efficace).\n",
    "\n",
    "en fait si on gère nous même l'accumulation des sorties, c'est plus coûteux avec fori_loop parce qu'on le fait manuellement\n",
    "\n",
    "dans le cas où les bornes sont statiques pour fori_loop, comme c'est implémenté en tant que scan c'est le même coût, par contre si les bornes ne sont pas statiques c'est là où le coût est différent\n",
    "\n",
    "##### checkpointing\n",
    "l'idée est dans le calcul du gradient reverse, on ne retienne pas les résultats des calculs intérmédiares dans le sens forward, mais on les recompute lorsqu'on fait la partie backward. Ca fait perdre plus de temps parcre qu'on refait les mêmes calculs mais gagner de la mémoire et potentiellement éviter un goulot d'étranglement.\n",
    "\n",
    "cons : il faut checkpointer sur les sous-fonctions de notre fonction principale pour skipper les étapes qu'il ne faut pas retenir\n",
    "\n",
    "il faut checkpointer sur les fonctions \"intérieures\" de la composition = celles qui sont au début dans l'ordre d'écriture du code\n",
    "\n",
    "**Policies** : en temps normal on doit modifier notre code pour ajouter des checkpointings mais il existe des \"policies\" pour dire ce qu'on checkpoint sans modifier le code de la fonction. C'est possible de mettre des \"names\" c'est comme des flags sur certaines parties des calculs pour les viser ou non dans la policy\n",
    "\n",
    "**Offload** : possible de \"déplacer\" certaines calculs du CPU au GPU\n",
    "\n",
    "**recursive checkpoints** idée pour avoir du $\\mathcal O (\\log_2 (D))$ au lieu de du $\\mathcal O (D)$ en mémoire pour les processus recursifs, l'idée est plus ou moins de faire une dicotomie sur les différentes étapes du proecessus recursif.\n",
    "\n",
    "##### Chekpointing, jit, scan\n",
    "En général, le checkpointing est inutile lorsqu'on fait le gradient d'un truc déjà jitté parce que XLA optimise déjà tout pour nous\n",
    "\n",
    "Mais une exeption pour `lax.scan` parce que l'opotimisation de cette fonction n'est pas la même pour la compute en mode forward et pour faire son gradient backward -> dans ce cas ça vaut le coup de mettre ces checkpoints dans la fonction qu'on met dans `lax.scan`.\n",
    "\n",
    "### Premelinary functions implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tridiag_solve_fori_loop(a: jnp.ndarray, b: jnp.ndarray, c: jnp.ndarray, f: jnp.ndarray) -> jnp.ndarray:\n",
    "    n, = a.shape\n",
    "    cff = 1.0 / b[0]\n",
    "    f = f.at[0].multiply(cff)\n",
    "    q = jnp.zeros(n)\n",
    "    q = q.at[0].set(-c[0] * cff)\n",
    "\n",
    "    def tri_forward_fori(k: int, x: jnp.ndarray):\n",
    "        f = x[0, :]\n",
    "        q = x[1, :]\n",
    "        cff = 1.0 / (b[k] + a[k] * q[k-1])\n",
    "        q = q.at[k].set(-cff * c[k])\n",
    "        f = f.at[k].set(cff * (f[k] - a[k] * f[k-1]))\n",
    "        return jnp.stack([f, q])\n",
    "    f_q = jnp.stack([f, q])\n",
    "    f_q = lax.fori_loop(1, n, tri_forward_fori, f_q)\n",
    "    f = f_q[0, :]\n",
    "    q = f_q[1, :]\n",
    "\n",
    "    def tri_reverse_fori(k: int, x: jnp.ndarray):\n",
    "        return x.at[n-1-k].add(q[n-1-k] * x[n-k])\n",
    "    x = lax.fori_loop(1, n, tri_reverse_fori, f)\n",
    "\n",
    "    return x\n",
    "\n",
    "def tridiag_solve_scan(a: jnp.ndarray, b: jnp.ndarray, c: jnp.ndarray, f: jnp.ndarray) -> jnp.ndarray:\n",
    "    def tri_forward_scan(carry: Tuple[float, float], x: jnp.ndarray):\n",
    "        f_im1, q_im1 = carry\n",
    "        a, b, c, f = x\n",
    "        cff = 1./(b+a*q_im1)\n",
    "        f_i = cff*(f-a*f_im1)\n",
    "        q_i = -cff*c\n",
    "        carry = f_i, q_i\n",
    "        return carry, carry\n",
    "    init = f[0]/b[0], -c[0]/b[0]\n",
    "    xs = jnp.stack([a, b, c, f])[:, 1:].T\n",
    "    _, (f, q) = lax.scan(tri_forward_scan, init, xs)\n",
    "    f = jnp.concat([jnp.array([init[0]]), f])\n",
    "    q = jnp.concat([jnp.array([init[1]]), q])\n",
    "\n",
    "    def tri_reverse_scan(carry: float, x: jnp.ndarray):\n",
    "        q_rev, f_rev = x\n",
    "        carry = f_rev + q_rev*carry\n",
    "        return carry, carry\n",
    "    init = f[-1]\n",
    "    xs = jnp.stack([q[::-1], f[::-1]])[:, 1:].T\n",
    "    _, x = lax.scan(tri_reverse_scan, init, xs)\n",
    "    x = jnp.concat([jnp.array([init]), x])\n",
    "\n",
    "    return x[::-1]\n",
    "\n",
    "def tridiag_solve_scan2(a: jnp.ndarray, b: jnp.ndarray, c: jnp.ndarray, f: jnp.ndarray) -> jnp.ndarray:\n",
    "    n, = a.shape\n",
    "    cff = 1.0 / b[0]\n",
    "    f = f.at[0].multiply(cff)\n",
    "    q = jnp.zeros(n)\n",
    "    q = q.at[0].set(-c[0] * cff)\n",
    "\n",
    "    # Forward pass with lax.scan\n",
    "    def tri_forward_scan(carry, k):\n",
    "        f, q = carry\n",
    "        cff = 1.0 / (b[k] + a[k] * q[k-1])\n",
    "        q = q.at[k].set(-cff * c[k])\n",
    "        f = f.at[k].set(cff * (f[k] - a[k] * f[k-1]))\n",
    "        return (f, q), None\n",
    "\n",
    "    (f, q), _ = lax.scan(tri_forward_scan, (f, q), jnp.arange(1, n))\n",
    "\n",
    "    # Reverse pass with lax.scan\n",
    "    def tri_reverse_scan(carry, k):\n",
    "        x = carry\n",
    "        x = x.at[n-1-k].add(q[n-1-k] * x[n-k])\n",
    "        return x, None\n",
    "\n",
    "    x, _ = lax.scan(tri_reverse_scan, f, jnp.arange(1, n))\n",
    "\n",
    "    return x\n",
    "\n",
    "def add_boundaries(vec_btm: float, vec_in: jnp.ndarray, vec_sfc: float) -> jnp.ndarray:\n",
    "    return jnp.concat([jnp.array([vec_btm]), vec_in, jnp.array([vec_sfc])])\n",
    "\n",
    "def diffusion_solver(ak: jnp.ndarray, hz: jnp.ndarray, f: jnp.ndarray, dt: float) -> jnp.ndarray:\n",
    "    a_in = -2.0 * dt * ak[1:-2] / (hz[:-2] + hz[1:-1])\n",
    "    c_in = -2.0 * dt * ak[2:-1] / (hz[2:] + hz[1:-1])\n",
    "    b_in = hz[1:-1] - a_in - c_in\n",
    "\n",
    "    c_btm = -2.0 * dt * ak[1] / (hz[1] + hz[0])\n",
    "    b_btm = hz[0] - c_btm\n",
    "\n",
    "    a_sfc = -2.0 * dt * ak[-2] / (hz[-2] + hz[-1])\n",
    "    b_sfc = hz[-1] - a_sfc\n",
    "\n",
    "    a = add_boundaries(0., a_in, a_sfc)\n",
    "    b = add_boundaries(b_btm, b_in, b_sfc)\n",
    "    c = add_boundaries(c_btm, c_in, 0.)\n",
    "\n",
    "    x = tridiag_solve_scan(a, b, c, f)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison of `lax.scan` and `lax.fori_loop` for the tridiagonal inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_vectors(key, n):\n",
    "    key, subkey = random.split(key)\n",
    "    a = random.uniform(subkey, (n,))\n",
    "    key, subkey = random.split(key)\n",
    "    b = random.uniform(subkey, (n,))\n",
    "    key, subkey = random.split(key)\n",
    "    c = random.uniform(subkey, (n,))\n",
    "    key, subkey = random.split(key)\n",
    "    f = random.uniform(subkey, (n,))\n",
    "    return a, b, c, f, key\n",
    "\n",
    "def benchmark(n: int, num_trials: int = 10):\n",
    "    tri_for_jit = jit(tridiag_solve_fori_loop)\n",
    "    tri_scan_jit = jit(tridiag_solve_scan)\n",
    "    tri_scan2_jit = jit(tridiag_solve_scan2)\n",
    "    key = random.PRNGKey(0)\n",
    "\n",
    "    print(f'Comparing strategies for tridiagonal inversion for vectors of size {n} and for a mean on {num_trials} trials\\n')\n",
    "\n",
    "    t = tt()\n",
    "    for _ in range(num_trials):\n",
    "        a, b, c, f, key = gen_rand_vectors(key, n)\n",
    "        _ = tridiag_solve_fori_loop(a, b, c, f)\n",
    "    for_time = (tt() - t) / num_trials\n",
    "    print(f'lax.fori_loop\\n{for_time}s\\n')\n",
    "\n",
    "    t = tt()\n",
    "    _ = tri_for_jit(a, b, c, f)\n",
    "    first_for_jit_time = tt() - t\n",
    "    print(f'lax.fori_loop + JIT (1st compilation) \\n{first_for_jit_time}s\\n')\n",
    "\n",
    "    t = tt()\n",
    "    for _ in range(num_trials):\n",
    "        a, b, c, f, key = gen_rand_vectors(key, n)\n",
    "        _ = tri_for_jit(a, b, c, f)\n",
    "    for_jit_time = (tt() - t) / num_trials\n",
    "    print(f'lax.fori_loop + JIT\\n{for_jit_time}s\\n')\n",
    "\n",
    "    t = tt()\n",
    "    for _ in range(num_trials):\n",
    "        a, b, c, f, key = gen_rand_vectors(key, n)\n",
    "        _ = tridiag_solve_scan(a, b, c, f)\n",
    "    scan_time = (tt() - t) / num_trials\n",
    "    print(f'lax.scan\\n{scan_time}s\\n')\n",
    "\n",
    "    t = tt()\n",
    "    _ = tri_scan_jit(a, b, c, f)\n",
    "    first_scan_jit_time = tt() - t\n",
    "    print(f'lax.scan + JIT (1st compilation) \\n{first_scan_jit_time}s\\n')\n",
    "\n",
    "    t = tt()\n",
    "    for _ in range(num_trials):\n",
    "        a, b, c, f, key = gen_rand_vectors(key, n)\n",
    "        _ = tri_scan_jit(a, b, c, f)\n",
    "    scan_jit_time = (tt() - t) / num_trials\n",
    "    print(f'lax.scan + JIT\\n{scan_jit_time}s\\n')\n",
    "\n",
    "    t = tt()\n",
    "    for _ in range(num_trials):\n",
    "        a, b, c, f, key = gen_rand_vectors(key, n)\n",
    "        _ = tridiag_solve_scan2(a, b, c, f)\n",
    "    scan_time = (tt() - t) / num_trials\n",
    "    print(f'lax.scan2\\n{scan_time}s\\n')\n",
    "\n",
    "    t = tt()\n",
    "    _ = tri_scan2_jit(a, b, c, f)\n",
    "    first_scan_jit_time = tt() - t\n",
    "    print(f'lax.scan2 + JIT (1st compilation) \\n{first_scan_jit_time}s\\n')\n",
    "\n",
    "    t = tt()\n",
    "    for _ in range(num_trials):\n",
    "        a, b, c, f, key = gen_rand_vectors(key, n)\n",
    "        _ = tri_scan2_jit(a, b, c, f)\n",
    "    scan_jit_time = (tt() - t) / num_trials\n",
    "    print(f'lax.scan2 + JIT\\n{scan_jit_time}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test results on Macbook Pro M3 Pro 18Go**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing strategies for tridiagonal inversion for vectors of size 1000 and for a mean on 100 trials\n",
      "\n",
      "lax.fori_loop\n",
      "0.04748637914657593s\n",
      "\n",
      "lax.fori_loop + JIT (1st compilation) \n",
      "0.04959678649902344s\n",
      "\n",
      "lax.fori_loop + JIT\n",
      "0.0007703304290771484s\n",
      "\n",
      "lax.scan\n",
      "0.03587670087814331s\n",
      "\n",
      "lax.scan + JIT (1st compilation) \n",
      "0.03472495079040527s\n",
      "\n",
      "lax.scan + JIT\n",
      "0.00022287845611572267s\n",
      "\n",
      "lax.scan2\n",
      "0.04254220008850098s\n",
      "\n",
      "lax.scan2 + JIT (1st compilation) \n",
      "0.03572678565979004s\n",
      "\n",
      "lax.scan2 + JIT\n",
      "0.00021483898162841797s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(1000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAIRE UN GRAPHE SELON N\n",
    "\n",
    "### Simplified model implementation\n",
    "We built this simplified model taking in account the same architecture than in tunax. We build the closure calculations to fit approximatelly the cost of calculation of k-epsilon, but the calculations have no sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid(eqx.Module):\n",
    "    nz: int\n",
    "    zr: jnp.ndarray\n",
    "    hz: jnp.ndarray\n",
    "    \n",
    "    def __init__(self, nz: int):\n",
    "        self.nz = nz\n",
    "        self.zr = jnp.linspace(-100, 0, nz)\n",
    "        self.hz = jnp.full(nz, 100/nz)\n",
    "\n",
    "class State(eqx.Module):\n",
    "    grid: Grid\n",
    "    t: jnp.ndarray\n",
    "\n",
    "class Trajectory(eqx.Module):\n",
    "    grid: Grid\n",
    "    time: jnp.ndarray\n",
    "    t: jnp.ndarray\n",
    "\n",
    "class CloState(eqx.Module):\n",
    "    grid: Grid\n",
    "    diff: jnp.ndarray\n",
    "\n",
    "\n",
    "    def __init__(self, grid: Grid):\n",
    "        self.grid = grid\n",
    "        self.diff = jnp.full(grid.nz+1, 1e-5)\n",
    "\n",
    "class CloParams(eqx.Module):\n",
    "    par1: float = 1.\n",
    "    par2: float = 1.\n",
    "    par3: float = 1.\n",
    "    par4: float = 1.\n",
    "    par5: float = 1.\n",
    "\n",
    "class Case(eqx.Module):\n",
    "    forc: float = .01\n",
    "\n",
    "def clo_step(state: State, clo_state: CloState, dt: float, clo_params: CloParams, case: Case)->CloParams: \n",
    "    t = state.t\n",
    "    diff = clo_state.diff\n",
    "\n",
    "    f1 = clo_params.par1 * jnp.sin(t) + clo_params.par2 * jnp.cos(diff[:-1])\n",
    "    f2 = clo_params.par3 * jnp.log1p(jnp.abs(diff[:-1]))\n",
    "    f3 = clo_params.par4 * t**2\n",
    "    convolution = 0.5 * (f3[:-1] + f3[1:])\n",
    "    f_combined = f1 + f2 + jnp.pad(convolution, (1, 0), mode='constant')\n",
    "    diff_new = diffusion_solver(diff, state.grid.hz, f_combined, dt)\n",
    "    diff_new = diffusion_solver(diff, state.grid.hz, diff_new, dt)\n",
    "    diff_new = diffusion_solver(diff, state.grid.hz, diff_new, dt)\n",
    "    diff_new = diffusion_solver(diff, state.grid.hz, diff_new, dt)\n",
    "    diff_new = diffusion_solver(diff, state.grid.hz, diff_new, dt)\n",
    "    diff_new = diffusion_solver(diff, state.grid.hz, diff_new, dt)\n",
    "    diff_new = diffusion_solver(diff, state.grid.hz, diff_new, dt)\n",
    "    diff_new = diffusion_solver(diff, state.grid.hz, diff_new, dt)\n",
    "    diff_new = diffusion_solver(diff, state.grid.hz, diff_new, dt)\n",
    "    diff_new = clo_params.par5*jnp.concat([diff_new, jnp.array([0.])])\n",
    "\n",
    "    clo_state = eqx.tree_at(lambda t: t.diff, clo_state, diff_new)\n",
    "    return clo_state\n",
    "\n",
    "class Model(eqx.Module):\n",
    "    nt: int\n",
    "    dt: float\n",
    "    n_out: int\n",
    "    init_state: State\n",
    "    case: Case\n",
    "\n",
    "    def step(self, clo_params: CloParams, state: State, clo_state: CloState) -> Tuple[State, CloState]:\n",
    "        # Extraction explicite des attributs nécessaires\n",
    "        grid = state.grid\n",
    "        hz = grid.hz\n",
    "        t = state.t\n",
    "        diff = clo_state.diff\n",
    "\n",
    "        # Mise à jour de `clo_state`\n",
    "        clo_state = clo_step(state, clo_state, self.dt, clo_params, self.case)\n",
    "\n",
    "        # Calcul de `dft` et de la nouvelle température\n",
    "        ft = jnp.zeros(state.t.shape[0] + 1)\n",
    "        ft = ft.at[-1].set(self.case.forc)\n",
    "        dft = hz * t + self.dt * (ft[1:] - ft[:-1])\n",
    "        new_t = diffusion_solver(diff, hz, dft, self.dt)\n",
    "\n",
    "        # Mise à jour de l'état avec `eqx.tree_at`\n",
    "        state = eqx.tree_at(lambda s: s.t, state, new_t)\n",
    "\n",
    "        return state, clo_state\n",
    "\n",
    "    # Compilation JAX-friendly\n",
    "    compiled_step = jit(step)\n",
    "    \n",
    "    def run_partial(self, clo_params: CloParams, state0: State, clo_state0: CloState, n_steps: int)->Tuple[State, CloState]:\n",
    "        state = state0\n",
    "        clo_state = clo_state0\n",
    "        for _ in range(n_steps):\n",
    "            state, clo_state = self.compiled_step(clo_params, state, clo_state)\n",
    "        return state, clo_state\n",
    "    \n",
    "    def run(self, clo_params: CloParams)->Trajectory:\n",
    "        state = self.init_state\n",
    "        clo_state = CloState(state.grid)\n",
    "        state_list = [state]\n",
    "        n_out = self.nt//self.n_out\n",
    "        for _ in range(n_out):\n",
    "            state, clo_state = self.run_partial(clo_params, state, clo_state, self.n_out)\n",
    "            state_list.append(state)\n",
    "        out_dt = self.n_out*self.dt\n",
    "        time = jnp.arange(0, (n_out+1)*out_dt,out_dt)\n",
    "        t_list = [s.t for s in state_list]\n",
    "        return Trajectory(state.grid, time, jnp.vstack(t_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5344898700714111\n",
      "0.7596253871917724\n"
     ]
    }
   ],
   "source": [
    "nz = 100\n",
    "grid = Grid(nz)\n",
    "t0 = jnp.linspace(15, 20, nz)\n",
    "init_state = State(grid, t0)\n",
    "case = Case()\n",
    "nt = 3000\n",
    "dt = 100\n",
    "n_out = 10\n",
    "\n",
    "model = Model(nt, dt, n_out, init_state, case)\n",
    "clo_params = CloParams()\n",
    "\n",
    "tic = tt()\n",
    "traj = model.run(clo_params)\n",
    "print(tt()-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study of the calculation of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class Observation(eqx.Module):\n",
    "    traj: Trajectory\n",
    "    case: Case\n",
    "    dt: float\n",
    "\n",
    "def loss(database: List[Observation], clo_params: CloParams):\n",
    "    s = 0\n",
    "    for obs in database:\n",
    "        traj = obs.traj\n",
    "        t0 = traj.t[0, :]\n",
    "        init_state = State(traj.grid, t0)\n",
    "        nt = int(float(traj.time[-1])/obs.dt)\n",
    "        n_out = int(float(traj.time[1]-traj.time[0])/obs.dt)\n",
    "        model = Model(nt, obs.dt, n_out, init_state, obs.case)\n",
    "\n",
    "        traj_model = model.run(clo_params)\n",
    "\n",
    "        s += jnp.sum((traj.t[-1, :] - traj_model.t[-1, :])**2)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3.5743142e-10, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nz = 100\n",
    "nt = 300\n",
    "dt = 100\n",
    "case = Case()\n",
    "grid = Grid(nz)\n",
    "traj = Trajectory(grid, jnp.linspace(0, (nt+1)*dt, nt+1), jnp.full((nt+1, nz), 15.))\n",
    "database = [Observation(traj, case, dt)]\n",
    "\n",
    "def loss_wrapped(x: jnp.ndarray):\n",
    "    return loss(database, CloParams(x[0], x[1], x[2], x[3], x[4]))\n",
    "\n",
    "loss_wrapped(jnp.array([0, 0, 0, 0, 0]))\n",
    "\n",
    "# from jax import grad\n",
    "\n",
    "# grad_loss = grad(loss_wrapped)\n",
    "\n",
    "\n",
    "# grad_loss(jnp.array([0., 0., 0., 0., 0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tunax-6Pt6l4qB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
